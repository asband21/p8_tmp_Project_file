from super_gradients.training import models
from ultralytics import YOLO
from pathlib import Path
from PIL import Image
import pillow_avif
import cv2

model = models.get("yolo_nas_m", pretrained=True)

# Ruta de la carpeta con las im치genes
video_path = Path(r"C:\Users\Eugen\Desktop\Documentos Curso 2do semestre\pbl\videos\friday\depth_17\rgb_output.mp4")
cap = cv2.VideoCapture(video_path)

# Obtener FPS y tama침o de salida
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Guardar video con detecciones
out = cv2.VideoWriter("output.mp4", cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    cv2.GaussianBlur(frame, (5, 5), 0)

    # Detecci칩n de personas
    results = model(frame)

    # Dibujar detecciones en el frame
    for result in results:
        for box in result.boxes:
            if result.names[int(box.cls)] == "person":  # Filtrar solo personas
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                confidence = box.conf[0] * 100  # Obtener confianza y convertir a porcentaje
                
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame, f"{confidence:.2f}%", (x1, y1 - 10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Mostrar el frame con detecciones
    cv2.imshow("Detecci칩n de Personas", frame)
    out.write(frame)  # Guardar el frame procesado

    # Salir con la tecla 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()